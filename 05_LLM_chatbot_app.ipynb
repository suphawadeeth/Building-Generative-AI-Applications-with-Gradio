{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAp9SSWneJ+xF5w3VwZHug",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suphawadeeth/Building-Generative-AI-Applications-with-Gradio/blob/main/05_LLM_chatbot_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chat with any LLM!** ðŸ’¬\n",
        "\n",
        "In this project, we are building a chatbot app with an open-source LLM **Falcon40B**.\n",
        "\n",
        "We'll be using an Inference Endpoint for **Falcon-40B-Instruct** , one of best ranking open source LLM on the ðŸ¤— Hugging Face Open LLM Leaderboard.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "Load your HF API key and relevant Python libraries\n"
      ],
      "metadata": {
        "id": "pYKGEMsquTpB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, ee are building a chat app with an open-source LLM."
      ],
      "metadata": {
        "id": "ISLTXiucyOjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import io\n",
        "import IPython.display\n",
        "from PIL import Image\n",
        "import base64\n",
        "import requests\n",
        "requests.adapters.DEFAULT_TIMEOUT = 60\n",
        "import requests, json\n",
        "from text_generation import Client\n",
        "from google.colab import userdata\n",
        "hf_api_key = userdata.get('HF_API_KEY')\n",
        "\n",
        "#FalcomLM-instruct endpoint on the text_generation library\n",
        "client = Client(userdata.get('HF_API_FALCOM_BASE'), headers={\"Authorization\": f\"Basic {hf_api_key}\"}, timeout=120)"
      ],
      "metadata": {
        "id": "NmF1dy2ZueAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Building an app to chat with any LLM**\n",
        "\n"
      ],
      "metadata": {
        "id": "9Nkeq7WA-Q6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Has math been invented or discovered?\"\n",
        "client.generate(prompt, max_new_tokens=256).generated_text"
      ],
      "metadata": {
        "id": "IPFoeGxI9-n7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "def generate(input, slider):\n",
        "    output = client.generate(input, max_new_tokens=slider).generated_text\n",
        "    return output\n",
        "\n",
        "demo = gr.Interface(fn=generate,\n",
        "                    inputs=[gr.Textbox(label=\"Prompt\"),\n",
        "                            gr.Slider(label=\"Max new tokens\",\n",
        "                                      value=20,\n",
        "                                      maximum=1024,\n",
        "                                      minimum=1)],\n",
        "                    outputs=[gr.Textbox(label=\"Completion\")])\n",
        "\n",
        "gr.close_all()\n",
        "demo.launch(share=True, server_port=int(os.environ['PORT1']))"
      ],
      "metadata": {
        "id": "eB-oSWOb9_nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the model does not have a memory. We could not lead a conversation.\n",
        "\n",
        "Therefore, to create a chatbot, we need to send a context of the question/answer and the follow up question to a model.\n",
        "\n",
        "Gradio can help with this task as a streamline conversation by sending the history conversation to the model.\n",
        "\n",
        "---\n",
        "\n",
        "**gr.Chatbot()**\n",
        "\n",
        "- `gr.Chatbot()` allows you to save the chat history (between the user and the LLM) as well as display the dialogue in the app.\n",
        "\n",
        "- Define your `function` to take in a `gr.Chatbot()` object.\n",
        "\n",
        "  - Within your defined fn function, append a tuple (or a list) containing the user message and the LLM's response:\n",
        "      \n",
        "      `chatbot_object.append( (user_message, llm_message) )`\n",
        "- Include the chatbot object in both the inputs and the outputs of the app."
      ],
      "metadata": {
        "id": "K9HBLkt3qdLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def respond(message, chat_history):\n",
        "        #No LLM here, just respond with a random pre-made message\n",
        "        bot_message = random.choice([\"Tell me more about it\",\n",
        "                                     \"Cool, but I'm not interested\",\n",
        "                                     \"Hmmmm, ok then\"])\n",
        "        chat_history.append((message, bot_message))\n",
        "        return \"\", chat_history\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
        "    msg = gr.Textbox(label=\"Prompt\")\n",
        "    btn = gr.Button(\"Submit\")\n",
        "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
        "\n",
        "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
        "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\n",
        "\n",
        "gr.close_all()\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "TWikMomWr4cH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Format the prompt with the chat historyÂ¶**\n",
        "\n",
        "- You can iterate through the chatbot object with a for loop.\n",
        "- Each item is a tuple containing the user message and the LLM's message.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "for turn in chat_history:\n",
        "    user_msg, bot_msg = turn\n",
        "    ...\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q0IyK8Efr88M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_chat_prompt(message, chat_history):\n",
        "    \"\"\"This function creates a history of the question-response and return them as a prompt\n",
        "    to feed the API\"\"\"\n",
        "    prompt = \"\"\n",
        "    for turn in chat_history:\n",
        "        user_message, bot_message = turn\n",
        "        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
        "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
        "    return prompt\n",
        "\n",
        "def respond(message, chat_history):\n",
        "    \"\"\"The function feed the model (API) using prompt (chat hisory) from the previous conversation\"\"\"\n",
        "        formatted_prompt = format_chat_prompt(message, chat_history) # we past formatted_prompt (the history conversation) to the API\n",
        "        bot_message = client.generate(formatted_prompt,\n",
        "                                     max_new_tokens=1024,\n",
        "                                     stop_sequences=[\"\\nUser:\", \"<|endoftext|>\"]).generated_text #to prevent the bot from impersonate the user message\n",
        "        chat_history.append((message, bot_message))\n",
        "        return \"\", chat_history\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
        "    msg = gr.Textbox(label=\"Prompt\")\n",
        "    btn = gr.Button(\"Submit\")\n",
        "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
        "\n",
        "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
        "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\n",
        "\n",
        "gr.close_all()\n",
        "demo.launch(share=True, server_port=int(os.environ['PORT3']))"
      ],
      "metadata": {
        "id": "jY0sCnxIsLhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Adding other advanced features**"
      ],
      "metadata": {
        "id": "DQmGCXOtsSy5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Streaming**\n",
        "\n",
        "If your LLM can provide its tokens one at a time in a stream, you can accumulate those tokens in the chatbot object.\n",
        "\n",
        "The for loop in the following function goes through all the tokens that are in the stream and appends them to the most recent conversational turn in the chatbot's message history."
      ],
      "metadata": {
        "id": "icWndUJ-sZOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_chat_prompt(message, chat_history, instruction):\n",
        "    prompt = f\"System:{instruction}\" #we add the system intruction here to create a persona for the bot\n",
        "    for turn in chat_history:\n",
        "        user_message, bot_message = turn\n",
        "        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
        "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
        "    return prompt\n",
        "\n",
        "def respond(message, chat_history, instruction, temperature=0.7):\n",
        "    prompt = format_chat_prompt(message, chat_history, instruction)\n",
        "    chat_history = chat_history + [[message, \"\"]]\n",
        "    stream = client.generate_stream(prompt,\n",
        "                                      max_new_tokens=1024,\n",
        "                                      stop_sequences=[\"\\nUser:\", \"<|endoftext|>\"],\n",
        "                                      temperature=temperature)\n",
        "                                      #stop_sequences to not generate the user answer\n",
        "    acc_text = \"\"\n",
        "    #Streaming the tokens\n",
        "    for idx, response in enumerate(stream):\n",
        "            text_token = response.token.text\n",
        "\n",
        "            if response.details:\n",
        "                return\n",
        "\n",
        "            if idx == 0 and text_token.startswith(\" \"):\n",
        "                text_token = text_token[1:]\n",
        "\n",
        "            acc_text += text_token\n",
        "            last_turn = list(chat_history.pop(-1))\n",
        "            last_turn[-1] += acc_text\n",
        "            chat_history = chat_history + [last_turn]\n",
        "            yield \"\", chat_history\n",
        "            acc_text = \"\""
      ],
      "metadata": {
        "id": "N3QNzEWQsiIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
        "    msg = gr.Textbox(label=\"Prompt\")\n",
        "    with gr.Accordion(label=\"Advanced options\",open=False):\n",
        "        system = gr.Textbox(label=\"System message\", lines=2, value=\"A conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.\")\n",
        "        temperature = gr.Slider(label=\"temperature\", minimum=0.1, maximum=1, value=0.7, step=0.1) #set how much variation of the answer e.g. if set to 0, the bot will only give the same answer everytime to the same question\n",
        "    btn = gr.Button(\"Submit\")\n",
        "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
        "\n",
        "    btn.click(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot])\n",
        "    msg.submit(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot]) #Press enter to submit\n",
        "\n",
        "gr.close_all()\n",
        "demo.queue().launch())"
      ],
      "metadata": {
        "id": "bYyHAVfXslR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice, in the cell above, you have used `demo.queue().launch()` instead of `demo.launch()`.\n",
        "\n",
        "\"queue\" helps you to boost up the performance for your demo. You can read [setting up a demo for maximum performance](https://www.gradio.app/guides/setting-up-a-demo-for-maximum-performance) for more details."
      ],
      "metadata": {
        "id": "ebKnwjfasr9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gr.close_all()"
      ],
      "metadata": {
        "id": "Zyzfib1Ns5S5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}